<!DOCTYPE html>
<html>
  <head>
    <title>A Study of Keyphrases Extraction & Generation</title>
    <meta charset="utf-8">
    <style>
      
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Open+Sans);
      @import url('https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700;800&display=swap');
      @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;1,100;1,300;1,400&display=swap');
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Roboto';
      }
      h1, h2, h3, h4 {
        font-family: 'Roboto';
        font-weight: 200;
        margin-bottom: -1%;
      }
      .remark-slide-content {
        padding: 1em 2em 1em 2em;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .remark-slide-content h3 { font-size: 1.2em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #888888;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 1em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
      .left-column h2:last-of-type, .left-column h3:last-child {
        color: #000;
      }
      .right-column {
        width: 78%;
        float: right;
        padding-top: 1em;
      }
      table {
        border: 0px solid #ccc;
        width: 100%;
        margin:0;
        padding:0;
        text-align: center;
        border-collapse: collapse;
        border-spacing: 0;
      }

      table tr {
        border: 1px solid #ddd;
        text-align: center;
        padding: 5px;
      }

      th {
        text-align: center;
      }

      table th, table td {
        border: 1px solid #ddd;
        padding: 1%;
        text-align: center;
        vertical-align: middle; 
      }

      #lefties {
        padding: 5px;
        text-align: left;
      }

      table th {
        text-transform: uppercase;
        font-size: 1em;
        font-family: 'Roboto';
        font-weight: 400;
        letter-spacing: 1px;
      }
    @media screen and (max-width: 500px) {
      table {
        border: 0;
      }

      table thead {
        display: none;
      }

      table tr {
        margin-bottom: 10px;
        display: block;
        border-bottom: 0px solid #ddd;
      }

      table td {
        display: block;
        border-bottom: 0px dotted #ccc;
      }

      table td:before {
        content: attr(data-label);
        float: left;
        text-transform: uppercase;
        font-weight: bold;
      }
    }
    </style>
  </head>
  <body>
    <textarea id="source">
class: middle, inverse
layout: true
---
## A Study of Keyphrases Extraction & Generation
### Eunhwan Park
### judepark@kookmin.ac.kr
### 20. 09. 04.
---
layout: false
.left-column[
  ### Baseline Experiments
]

.right-column[
Baseline experiments is consist of 3 models. 
- One Size Does Not Fit All (CatSeqD)
- Exclusive Hierarchical Decoding for Deep Keyphrase Generation (ExHiRD)
- BERT-based Span Extraction (BERT-SE)
]

---
layout: false
.left-column[
  ### Baseline Experiments
]
.right-column[
One Size Does Not Fit All (CatSeqD)
- Semantic Coverage
- Orthogonal Regularization
- Exhaustive Decoding

Results

|  |      KP20K      |  Inspec | SemEval
|----------|-------------|------|--
| F1@5 |  0|0|0
| F1@10 |    0   |   0 | 0 

]
---
layout: false
.left-column[
  ### Baseline Experiments
]
.right-column[
Exclusive Hierarchical Decoding for Deep Keyphrase Generation (ExHiRD)
- Hierarchical Decoding Process

Results

|  |      KP20K      |  Inspec | SemEval
|----------|-------------|------|--
| F1@5 |  0| 0 | 0
| F1@10 |    0   |   0 | 0 

]
---
layout: false
.left-column[
  ### Baseline Experiments
]
.right-column[
BERT-based Span Extraction (BERT-SE)
- BERT Embedding
- Span Extraction Approach

Results

|  |      KP20K      
|----------|-------------|
| F1@span_start |  45.61  | 
| F1@span_end | 45.64  |
]

---
layout: false
.left-column[
  ### Baseline Experiments
  ### GCN-based Representation
]
.right-column[
Training high-level hidden feature for nodes through Graph Convolutional Network (GCN)

Inputs - Adjacency Matrix ð´, Node-Feature Matrix ð‘‹

GCN layer formula is follows as
$$H^{(l+1)} = \sigma(AH^lW^l+b^l)$$

Main process is follows as:
$$\text{Embed(x)} \rightarrow \text{GCN Layer} \rightarrow \text{Graph Representation G}$$

Reference
- [DivGraphPointer: A Graph Pointer Network for Extracting Diverse Keyphrases](https://arxiv.org/abs/1905.07689)
- [Graph Convolutional Network for Text Classification](https://arxiv.org/abs/1809.05679)
]
---
layout: false
.left-column[
  ### Baseline Experiments
  ### GCN-based Representation
]
.right-column[
Graph Construction
- The basic assumption is that the closer two words in a sentence, the stronger their relation.

How to construct adjacency matrix?
```python
adj = torch.zeros([n, n])

for i, offset_i in enumerate(tokens):
    for j, offset_j in enumerate(tokens):
        if i == j: continue
        adj[i][j] = np.maximum(0, (1 / (offset_i - offset_j)))

adj += torch.eye(n, n)

return normalize_graph(adj)
```

But, usually construct adjacency matrix through dependency parse tree.
]
---
layout: false
.left-column[
  ### Baseline Experiments
  ### GCN-based Representation
]
.right-column[
Build encoder as GCN, not RNN-based!

```python
class GCNLayer(nn.Module):
    def __init__(self, input_dim: int, output_dim: int) -> None:
        super(GCNLayer, self).__init__()
        self.W_f = nn.Parameter(torch.randn(input_dim, output_dim))
        self.W_b = nn.Parameter(torch.randn(input_dim, output_dim))
        self.W = nn.Parameter(torch.randn(input_dim, output_dim))

    def forward(self,
                x: torch.Tensor,
                a_f: torch.Tensor,
                a_b: torch.Tensor):
        h_f = torch.bmm(a_f, x).matmul(self.W_f)
        h_b = torch.bmm(a_b, x).matmul(self.W_b)
        h_w = torch.matmul(x, self.W)

        f_h = h_f + h_b + h_w
        return x + (f_h * torch.sigmoid(f_h))
```
Advantage of GCN-based Encoder:
- We can explicitly leverage the short- and long-term dependency between words.

But, there is some issues:
- Restrict the output to the nodes of the graph.
]
---
layout: false

.left-column[
  ### Baseline Experiments
  ### GCN-based Representation
]

.right-column[

Build encoder as combining GCN and RNN.

<center><img src="./rnn-gcn-figure.png" width="80%"></center>
<center><p>Figure 1. Illustration of RNN + GCN</p></center>
]
---
layout: false
class: middle, inverse

# <center>Thank you</center>

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script type="text/javascript" src="http://livejs.com/live.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script>
      var slideshow = remark.create();
      
      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>